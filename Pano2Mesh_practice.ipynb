{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import open3d as o3d\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from utils.functions import (\n",
    "    rot_z_world_to_cam, \n",
    "    resize_image_with_aspect_ratio, \n",
    "    tensor_to_pil\n",
    ")\n",
    "from utils.common_utils import (\n",
    "    visualize_depth_numpy,\n",
    "    save_rgbd,\n",
    ")\n",
    "from modules.mesh_fusion.render import features_to_world_space_mesh\n",
    "from modules.geo_predictors.PanoFusionDistancePredictor import PanoFusionDistancePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pano2Mesh(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.device = \"cuda:0\"\n",
    "\n",
    "        self.fov = 90\n",
    "        self.H, self.W = 512, 512\n",
    "        self.pano_width = 1024 * 2\n",
    "        self.pano_height = 512 * 2\n",
    "\n",
    "        self.pano_center_offset = (-0.2,0.3)\n",
    "        self.pose_scale = 0.6\n",
    "\n",
    "        # initialize\n",
    "        self.rendered_depth = torch.zeros((self.H, self.W), device=self.device) \n",
    "        self.inpaint_mask = torch.ones((self.H, self.W), device=self.device, dtype=torch.bool)  \n",
    "        self.vertices = torch.empty((3, 0), device=self.device, requires_grad=False)\n",
    "        self.colors = torch.empty((3, 0), device=self.device, requires_grad=False)\n",
    "        self.faces = torch.empty((3, 0), device=self.device, dtype=torch.long, requires_grad=False)\n",
    "        self.pix_to_face = None\n",
    "\n",
    "        # create result dir\n",
    "        self.save_path = f'output/Pano2Room-results'\n",
    "\n",
    "    def load_pano(self):\n",
    "        image_path = f\"input/input_panorama.png\"\n",
    "        image = Image.open(image_path)\n",
    "        if image.size[0] < image.size[1]:\n",
    "            image = image.transpose(Image.TRANSPOSE)\n",
    "        image = resize_image_with_aspect_ratio(image, new_width=self.pano_width)\n",
    "        panorama_tensor = torch.tensor(np.array(image))[...,:3].permute(2,0,1).unsqueeze(0).float()/255\n",
    "        panorama_image_pil = tensor_to_pil(panorama_tensor)\n",
    "\n",
    "        depth_scale_factor = 3.4092\n",
    "\n",
    "        # get panofusion_distance\n",
    "        pano_fusion_distance_predictor = PanoFusionDistancePredictor()\n",
    "        depth = pano_fusion_distance_predictor.predict(panorama_tensor.squeeze(0).permute(1,2,0)) #input:HW3\n",
    "        depth = depth/depth.max() * depth_scale_factor\n",
    "        print(f\"pano_fusion_distance...[{depth.min(), depth.mean(),depth.max()}]\")\n",
    "        \n",
    "        return panorama_tensor, depth# panorama_tensor:BCHW, depth:HW\n",
    "\n",
    "    def load_camera_poses(self, pano_center_offset=[0,0]):\n",
    "        subset_path = f'input/Camera_Trajectory' # initial 6 poses are cubemaps poses\n",
    "        files = os.listdir(subset_path)\n",
    "        self.scene_depth_max = 4.0228885328450446\n",
    "        pano_pose_44 = None\n",
    "\n",
    "        pose_files = [f for f in files if f.startswith('camera_pose')]\n",
    "        pose_files = sorted(pose_files)\n",
    "        poses_name = pose_files\n",
    "        poses = []\n",
    "\n",
    "        for i, pose_name in enumerate(poses_name):\n",
    "            with open(f'{subset_path}/{pose_name}', 'r') as f: \n",
    "                lines = f.readlines()\n",
    "            pose_44 = []\n",
    "            for line in lines:\n",
    "                pose_44 += line.split()\n",
    "            pose_44 = np.array(pose_44).reshape(4, 4).astype(float)\n",
    "            if pano_pose_44 is None:\n",
    "                pano_pose_44 = pose_44.copy()\n",
    "                pano_pose_44_cubemaps = pose_44.copy()\n",
    "                pano_pose_44[0,3] += pano_center_offset[0]\n",
    "                pano_pose_44[2,3] += pano_center_offset[1]\n",
    "            \n",
    "            if i < 6:\n",
    "                pose_relative_44 = pose_44 @ np.linalg.inv(pano_pose_44_cubemaps)  \n",
    "            else:\n",
    "                ### convert gt_pose to gt_relative_pose with pano_pose\n",
    "                pose_relative_44 = pose_44 @ np.linalg.inv(pano_pose_44)\n",
    "\n",
    "            pose_relative_44 = np.vstack((-pose_relative_44[0:1,:], -pose_relative_44[1:2,:], pose_relative_44[2:3,:], pose_relative_44[3:4,:]))\n",
    "            pose_relative_44 = pose_relative_44 @ rot_z_world_to_cam(180).cpu().numpy()\n",
    "\n",
    "            pose_relative_44[:3,3] *= self.pose_scale\n",
    "            poses += [torch.tensor(pose_relative_44).float()] # w2c\n",
    "\n",
    "        return pano_pose_44, poses\n",
    "\n",
    "    def find_depth_edge(self, depth, dilate_iter=0):\n",
    "        gray = (depth/depth.max() * 255).astype(np.uint8)\n",
    "        edges = cv2.Canny(gray, 60, 150)\n",
    "        if dilate_iter > 0:\n",
    "            kernel = np.ones((3, 3), np.uint8)\n",
    "            edges = cv2.dilate(edges, kernel, iterations=dilate_iter)\n",
    "        return edges    \n",
    "\n",
    "    def rgb_to_mesh(self, \n",
    "                    rgb, \n",
    "                    depth, \n",
    "                    world_to_cam=None, \n",
    "                    mask=None, \n",
    "                    pix_to_face=None, \n",
    "                    using_distance_map=False):\n",
    "        predicted_depth = depth.cuda()\n",
    "        rgb = rgb.squeeze(0).cuda()\n",
    "        if world_to_cam is None:\n",
    "            world_to_cam = torch.eye(4, dtype=torch.float32)\n",
    "        world_to_cam = world_to_cam.cuda()\n",
    "        if pix_to_face is not None:\n",
    "            self.pix_to_face = pix_to_face\n",
    "        if mask is None:\n",
    "            self.inpaint_mask = torch.ones_like(predicted_depth)\n",
    "        else:\n",
    "            self.inpaint_mask = mask\n",
    "\n",
    "        if self.inpaint_mask.sum() == 0:\n",
    "            return\n",
    "        \n",
    "        vertices, faces, colors = features_to_world_space_mesh(\n",
    "            colors=rgb,\n",
    "            depth=predicted_depth,\n",
    "            fov_in_degrees=self.fov,\n",
    "            world_to_cam=world_to_cam,\n",
    "            mask=self.inpaint_mask,\n",
    "            pix_to_face=self.pix_to_face,\n",
    "            faces=self.faces,\n",
    "            vertices=self.vertices,\n",
    "            using_distance_map=using_distance_map,\n",
    "            edge_threshold=0.05\n",
    "        )\n",
    "\n",
    "        faces += self.vertices.shape[1] \n",
    "\n",
    "        self.vertices_restore = self.vertices.clone()\n",
    "        self.colors_restore = self.colors.clone()\n",
    "        self.faces_restore = self.faces.clone()\n",
    "\n",
    "        self.vertices = torch.cat([self.vertices, vertices], dim=1)\n",
    "        self.colors = torch.cat([self.colors, colors], dim=1)\n",
    "        self.faces = torch.cat([self.faces, faces], dim=1)\n",
    "\n",
    "    def pano_distance_to_mesh(self, \n",
    "                              pano_rgb, \n",
    "                              pano_distance, \n",
    "                              depth_edge_inpaint_mask, \n",
    "                              pose=None):\n",
    "        self.rgb_to_mesh(pano_rgb, pano_distance, mask=depth_edge_inpaint_mask, using_distance_map=True, world_to_cam=pose)\n",
    "\n",
    "    def torch_to_o3d_mesh(self):\n",
    "        # vertices를 Nx3 형태로 변환\n",
    "        # faces를 Nx3 형태로 변환\n",
    "        vertices_np = self.vertices.permute(1, 0).cpu().numpy()\n",
    "        faces_np = self.faces.T.cpu().numpy()\n",
    "        # Open3D TriangleMesh 객체 생성\n",
    "\n",
    "        mesh = o3d.geometry.TriangleMesh()\n",
    "        mesh.vertices = o3d.utility.Vector3dVector(vertices_np)\n",
    "        mesh.triangles = o3d.utility.Vector3iVector(faces_np)\n",
    "\n",
    "        # 색상이 있는 경우 설정\n",
    "        if self.colors is not None:\n",
    "            colors_np = self.colors.permute(1, 0).cpu().numpy()\n",
    "            mesh.vertex_colors = o3d.utility.Vector3dVector(colors_np)\n",
    "        \n",
    "        return mesh\n",
    "\n",
    "    def get_mash(self):\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        self.pano_pose, self.poses = self.load_camera_poses(self.pano_center_offset)\n",
    "        pano_rgb, pano_depth = self.load_pano() # pano_depth is prediected by PanoFusionDistancePredictor\n",
    "        panorama_tensor, init_depth = pano_rgb.squeeze(0).cuda(), pano_depth.cuda()\n",
    "\n",
    "        # depth_edge -> Canny edge\n",
    "        depth_edge = self.find_depth_edge(init_depth.cpu().detach().numpy(), dilate_iter=1)\n",
    "        depth_edge_pil = Image.fromarray(depth_edge)\n",
    "        depth_pil = Image.fromarray(visualize_depth_numpy(init_depth.cpu().detach().numpy())[0].astype(np.uint8))\n",
    "        _, _ = save_rgbd(depth_pil, depth_edge_pil, f'depth_edge', 0, self.save_path) \n",
    "        # Contribution of the paper -> depth edge mask M_D (3.1 Pano to Mesh)\n",
    "        # It enhances the smoothness of edge in occluded regions\n",
    "        depth_edge_inpaint_mask = ~(torch.from_numpy(depth_edge).cuda().bool()) \n",
    "\n",
    "        # Pano2Mesh\n",
    "        self.pano_distance_to_mesh(panorama_tensor, init_depth, depth_edge_inpaint_mask)\n",
    "\n",
    "        # Open3D Mesh 객체 생성\n",
    "        mesh = self.torch_to_o3d_mesh()\n",
    "        return mesh\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 2000/2000 [00:40<00:00, 49.67it/s]\n",
      "100%|██████████| 2000/2000 [00:46<00:00, 42.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pano_fusion_distance...[(tensor(0.5257), tensor(0.9973), tensor(3.4092))]\n"
     ]
    }
   ],
   "source": [
    "p2m = Pano2Mesh()\n",
    "mesh = p2m.get_mash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh 저장 완료: /home/Pano2Room/output/Pano2Room-results/mesh.ply\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/Pano2Room/output/Pano2Room-results/mesh.ply\"\n",
    "o3d.io.write_triangle_mesh(file_path, mesh)\n",
    "print(f\"Mesh 저장 완료: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
